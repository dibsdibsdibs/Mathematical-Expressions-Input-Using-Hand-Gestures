{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8-FHwBoSR5v"
      },
      "source": [
        "#Final Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5pfZQBwSWMm"
      },
      "source": [
        "Details:\n",
        "*   Sign Language Dataset for ASL Numbers and Operators\n",
        "*   CNN for recognizing hand sign\n",
        "*   OpenCV to capture hand gestures\n",
        "*   use trained model to recognize hand gestures\n",
        "*   recognized signs to form mathematical expression\n",
        "*   evaluate expression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lzSkJWwmSaUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e98fbc-1fc0-42aa-c2c0-ed175bc03c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.4)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.14)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.7)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.25.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.14 protobuf-4.25.3 sounddevice-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas\n",
        "!pip install opencv-python\n",
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JzUZOqwv3gdG"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import mediapipe as mp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsILoqdsz5Ke",
        "outputId": "536f98f1-eba1-419e-a860-293f18732181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: divinelavente\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/rayeed045/american-sign-language-digit-dataset\n",
            "Downloading american-sign-language-digit-dataset.zip to ./american-sign-language-digit-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 288M/288M [00:04<00:00, 64.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# od.download(\"https://www.kaggle.com/datasets/gti-upm/leapgestrecog/data\")\n",
        "od.download(\"https://www.kaggle.com/datasets/rayeed045/american-sign-language-digit-dataset/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tEcfyMFxRy4",
        "outputId": "caddb81d-2223-4d4b-e7df-3364d28fa874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset deleted successfully.\n"
          ]
        }
      ],
      "source": [
        "# import shutil\n",
        "# directory_to_delete = '/content/'\n",
        "# shutil.rmtree(directory_to_delete)\n",
        "# print(\"Dataset deleted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkZF_gLdlljP",
        "outputId": "4152cba1-99a7-4505-e30c-ac663a90c5ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "Contents of the current directory: ['.config', 'american-sign-language-digit-dataset', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "# Check the current working directory\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "# List the contents of the current working directory\n",
        "print(\"Contents of the current directory:\", os.listdir())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hdwyeASv5qxI"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/american-sign-language-digit-dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4vb_rRaJU1o",
        "outputId": "27ba0fe5-7a1b-4efc-914a-0feea6d809bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keypoints extracted:  4303\n",
            "Labels extracted:  4303\n"
          ]
        }
      ],
      "source": [
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.7)\n",
        "\n",
        "def extract_keypoints(image):\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    result = hands.process(image_rgb)\n",
        "    if result.multi_hand_landmarks:\n",
        "        hand_landmarks = result.multi_hand_landmarks[0]\n",
        "        keypoints = []\n",
        "        for lm in hand_landmarks.landmark:\n",
        "            keypoints.extend([lm.x, lm.y, lm.z])\n",
        "        return keypoints\n",
        "    return None\n",
        "\n",
        "imagepaths = []\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    for file in files:\n",
        "        if file.endswith(('.png', '.jpg', '.jpeg')):  # Add any other image extensions if necessary\n",
        "            imagepaths.append(os.path.join(root, file))\n",
        "\n",
        "X = []  # Keypoints data\n",
        "y = []  # Labels\n",
        "\n",
        "for path in imagepaths:\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        print(f\"Error: The image at path '{path}' could not be read.\")\n",
        "        continue\n",
        "    keypoints = extract_keypoints(img)\n",
        "    if keypoints:\n",
        "        X.append(keypoints)\n",
        "        try:\n",
        "            folder_name = path.split(\"/\")[-2]\n",
        "            label = ''.join(filter(str.isdigit, folder_name))\n",
        "            y.append(int(label))\n",
        "        except ValueError as ve:\n",
        "            print(f\"Error: Unable to convert label to integer in path '{path}': {ve}\")\n",
        "            continue\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"Keypoints extracted: \", len(X))\n",
        "print(\"Labels extracted: \", len(y))\n",
        "\n",
        "# Save keypoints and labels if needed\n",
        "np.save('X_keypoints.npy', X)\n",
        "np.save('y_labels.npy', y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "594yQljySfuI",
        "outputId": "de45376f-36fb-4556-ee41-07d82e883944"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 2304000000 into shape (20000,120,320,1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-aae612913b6f>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Turn X and y into np.array to speed up train_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Needed to reshape so CNN knows it's different images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 2304000000 into shape (20000,120,320,1)"
          ]
        }
      ],
      "source": [
        "\n",
        "# # Collecting all image paths\n",
        "# imagepaths = []\n",
        "# for root, dirs, files in os.walk(base_path):\n",
        "#     for file in files:\n",
        "#         if file.endswith(('.png', '.jpg', '.jpeg')):  # Add any other image extensions if necessary\n",
        "#             imagepaths.append(os.path.join(root, file))\n",
        "\n",
        "# X = []  # Image data\n",
        "# y = []  # Labels\n",
        "\n",
        "# # Loop through image paths to load images and labels into arrays\n",
        "# for path in imagepaths:\n",
        "#     img = cv2.imread(path)  # Reads image and returns np.array\n",
        "#     if img is None:\n",
        "#         print(f\"Error: The image at path '{path}' could not be read.\")\n",
        "#         continue\n",
        "#     try:\n",
        "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Converts into the correct colorspace (GRAY)\n",
        "#         img = cv2.resize(img, (320, 120))  # Reduce image size so training can be faster\n",
        "#         X.append(img)\n",
        "\n",
        "#         # Processing label in image path\n",
        "#         category = path.split(\"/\")[-2]\n",
        "#         label = int(category.split(\"_\")[0][1])  # Adjusted for proper directory depth\n",
        "#         y.append(label)\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error processing image at path '{path}': {e}\")\n",
        "#         continue\n",
        "\n",
        "# # Turn X and y into np.array to speed up train_test_split\n",
        "# X = np.array(X, dtype=\"uint8\")\n",
        "# X = X.reshape(len(X), 120, 320, 1)  # Needed to reshape so CNN knows it's different images\n",
        "# y = np.array(y)\n",
        "\n",
        "# print(\"Images loaded: \", len(X))\n",
        "# print(\"Labels loaded: \", len(y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTD6WChtpQZ6",
        "outputId": "84453272-9dd5-4566-ed77-266b7c3579db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 2.1677 - accuracy: 0.2205 - val_loss: 1.8425 - val_accuracy: 0.4669\n",
            "Epoch 2/20\n",
            "108/108 [==============================] - 0s 3ms/step - loss: 1.6690 - accuracy: 0.4262 - val_loss: 1.2044 - val_accuracy: 0.7666\n",
            "Epoch 3/20\n",
            "108/108 [==============================] - 0s 3ms/step - loss: 1.1712 - accuracy: 0.6238 - val_loss: 0.7015 - val_accuracy: 0.9024\n",
            "Epoch 4/20\n",
            "108/108 [==============================] - 0s 3ms/step - loss: 0.8307 - accuracy: 0.7377 - val_loss: 0.4015 - val_accuracy: 0.9524\n",
            "Epoch 5/20\n",
            "108/108 [==============================] - 0s 2ms/step - loss: 0.5974 - accuracy: 0.8234 - val_loss: 0.2328 - val_accuracy: 0.9907\n",
            "Epoch 6/20\n",
            "108/108 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.8774 - val_loss: 0.1540 - val_accuracy: 0.9942\n",
            "Epoch 7/20\n",
            "108/108 [==============================] - 0s 3ms/step - loss: 0.3582 - accuracy: 0.9001 - val_loss: 0.0986 - val_accuracy: 0.9942\n",
            "Epoch 8/20\n",
            "108/108 [==============================] - 0s 2ms/step - loss: 0.2843 - accuracy: 0.9291 - val_loss: 0.0675 - val_accuracy: 0.9965\n",
            "Epoch 9/20\n",
            "108/108 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.9346 - val_loss: 0.0536 - val_accuracy: 0.9965\n",
            "Epoch 10/20\n",
            "108/108 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.9463 - val_loss: 0.0412 - val_accuracy: 0.9965\n",
            "Epoch 11/20\n",
            "108/108 [==============================] - 0s 3ms/step - loss: 0.1897 - accuracy: 0.9524 - val_loss: 0.0341 - val_accuracy: 0.9965\n",
            "Epoch 12/20\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.1717 - accuracy: 0.9529 - val_loss: 0.0297 - val_accuracy: 0.9977\n",
            "Epoch 13/20\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.1589 - accuracy: 0.9643 - val_loss: 0.0264 - val_accuracy: 0.9965\n",
            "Epoch 14/20\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.1433 - accuracy: 0.9643 - val_loss: 0.0284 - val_accuracy: 0.9954\n",
            "Epoch 15/20\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.1216 - accuracy: 0.9730 - val_loss: 0.0229 - val_accuracy: 0.9977\n",
            "Epoch 16/20\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9715 - val_loss: 0.0194 - val_accuracy: 0.9977\n",
            "Epoch 17/20\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.1113 - accuracy: 0.9733 - val_loss: 0.0182 - val_accuracy: 0.9977\n",
            "Epoch 18/20\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.1017 - accuracy: 0.9736 - val_loss: 0.0181 - val_accuracy: 0.9977\n",
            "Epoch 19/20\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.1036 - accuracy: 0.9709 - val_loss: 0.0155 - val_accuracy: 0.9977\n",
            "Epoch 20/20\n",
            "108/108 [==============================] - 0s 3ms/step - loss: 0.0918 - accuracy: 0.9773 - val_loss: 0.0162 - val_accuracy: 0.9977\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0162 - accuracy: 0.9977\n",
            "Test accuracy: 0.9976771473884583\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# load keypoints and labels\n",
        "X_keypoints = np.load('X_keypoints.npy')\n",
        "y_labels = np.load('y_labels.npy')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_keypoints, y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a simple neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(63,)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')  # 10 classes for digits 0-9\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "pZZgOHpfuts0",
        "outputId": "275558b6-d9ba-47ac-e6c8-7c686ede843f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-451c0da75a47>:4: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  save_model(model, 'hand_gesture_model.h5')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5b168708-471b-4faa-8005-35510ca0ab63\", \"hand_gesture_model.h5\", 240128)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras.models import save_model\n",
        "from google.colab import files\n",
        "\n",
        "save_model(model, 'hand_gesture_model.h5')\n",
        "files.download('hand_gesture_model.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}